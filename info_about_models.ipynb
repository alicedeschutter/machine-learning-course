{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08501c4",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72423142",
   "metadata": {},
   "source": [
    "= the training data you feed to the algorithm includes the desired solutions (called labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac359ed",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab89e29",
   "metadata": {},
   "source": [
    "Many fancy statistical learning approaches can be seen as generalizations or extensions of linear\n",
    "regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d72897",
   "metadata": {},
   "source": [
    "### Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c649ad",
   "metadata": {},
   "source": [
    "A method for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. A **regression of Y on X** is expressed as: \n",
    "\n",
    "$$ \\large Y \\approx \\beta_{0} + \\beta_{1}X $$\n",
    "\n",
    "Where $\\beta_{0}$ and $\\beta_{1}$ are known as the model coefficients or parameters. \n",
    "\n",
    "To find $\\hat{y}$ the prediction of $Y$ on the basis of $X = x$, the equation below is used: \n",
    "\n",
    "$$ \\large \\hat{y} \\approx \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x $$ \n",
    "\n",
    "The hat symbol denotes the estimated value for an unknown parameter, or the predicted value of the response. The equation is known as the **least square line** since $\\beta_{0}$ and $\\beta_{1}$ are usually found by **minimizing the least squares criterion** (/minimizing the **Residual Sum of Squares (RSS)**):\n",
    "\n",
    "$$ RSS = (y_{1} - \\hat{y_{1}})^2 + (y_{2} - \\hat{y_{2}})^2 ... + (y_{n} - \\hat{y_{n}})^2 =(y_{1} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{1})^2 + (y_{2} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{2})^2 ... + (y_{n} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{n})^2\n",
    " $$\n",
    " \n",
    "   The RSS is used as a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. \n",
    "    \n",
    "The equation that is used to express the best linear approximation to the true relationship between X and Y is known as the **population regression line**:\n",
    "\n",
    "$$ \\large Y = \\beta_{0} + \\beta_{1}X + \\epsilon$$\n",
    "\n",
    "In real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8ee4d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dbdbc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Algorithm that makes classification predictions about your desired data points based on the assumption that nearby data points are similar to your test point.\n",
    "\n",
    "The k-nearest neighbors algorithm hinges on data points being close together. This becomes challenging as the number of dimensions increases, referred to as the “Curse of Dimensionality.” It’s especially hard for the k-nearest neighbors algorithm it requires two points to be very close on every axis, and adding a new dimension creates another opportunity for points to be farther apart. As the number of dimensions increases, the closest distance between two points approaches the average distance between points, eradicating the ability of the k-nearest neighbors algorithm to provide valuable predictions.\n",
    "\n",
    "To overcome this challenge, you can add more data to the data set. By doing so you add density to the data space, bringing the nearest points closer together and returning the ability of the k-nearest neighbors algorithm to provide valuable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9458e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6a774",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decision trees involve segmenting the predictor space into a number of distinct and non-overlapping regions (R1, R2, ... RJ). Each split of the domain is aligned with one of the feature axes (in theory they could have any shape but \"rectangles\" are chosen for simplicity's sake and ease of interpretation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c651c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34032c1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MEAN** of all training observations in RJ (the mean of the training observation in each partition is represented by a leaf).\n",
    "\n",
    "\n",
    "- The goal is to find boxes that minimize **the Residual Sum of Squares (RSS)**. \n",
    "- The **RSS** is computed by summing, for each test observation and across all partitions of the feature space, the squared difference of the response $y_{i}$ of a particular testing observation with the mean response of the training observation within the $j_{th}$ region. \n",
    "\n",
    "$$ \\large RSS = \\sum\\limits_{j=1} ^{J} \\sum\\limits_{i \\in R_{j}} (y_{i} - \\hat{y}_{R_{J}})^2 $$\n",
    "\n",
    "\n",
    "- Since it is computationally expensive to consider all possible partitions of the feature space into J rectangles, minimizing the RSS is done with **the Recursive Binary Splitting approach (RBS)**.\n",
    "\n",
    "\n",
    "- In one sentence: the RBS approach helps construct a tree by considering all features (X1, ... Xp) and all possible values of the cutpoint s for each of the features, and then choosing at each node the feature that best splits the data. It is said to be: \n",
    "    - *top down* because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space (each split --> new branch). \n",
    "    - *greedy* because at each step of the building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "\n",
    "- The RSS in greater detail: \n",
    "\n",
    "  For any j and s, a pair of half planes is defined (1). We then seek the values of j and s that minimize the equation (2) . \n",
    "  This process is repeated. However, each time, instead of splitting the entire predictor/feature space, only one of the two previously identified regions is split. The process continues until a stopping criterion is reached (i.e. until no region contains more than five observations).\n",
    "    1. \n",
    "    $$ \\large R_{1(j,s)} = \\{X | X_{j} < s\\} $$\n",
    "    $$ \\large R_{2(j,s)} = \\{X | X_{j} \\geqslant s\\} $$\n",
    "    \n",
    "    2. \n",
    "    \n",
    "$$ \\large\\sum\\limits_{i: x_{i} \\in R_{1(j,s)}} (y_{i} - \\hat{y}_{R_{1}})^2 + \\sum\\limits_{i: x_{i} \\in R_{2(j,s)}} (y_{i} - \\hat{y}_{R_{2}})^2 $$\n",
    "\n",
    "- In order to **NOT overfit** the data you could:\n",
    "\n",
    "    - Alternative 1: Build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. However, this strategy is too short-sighted since a \"worthless\" split might be followed by a \"good\" split later on.\n",
    "   \n",
    "    - Alternative 2: Grow a very large tree and then prune it back in order to obtain a subtree.\n",
    "    \n",
    "   **Cost complexity pruning** : introduces an additional tuning parameter ($\\alpha$) that balances the depth of the tree and the goodness of fit to training data. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_{0}$ such that:\n",
    "   \n",
    "       $$ \\large \\sum\\limits_{n=1} ^{|T|} \\sum\\limits_{x_{i} \\in R_{J}} (y_{i} - \\hat{y}_{R_{J}})^2 + \\alpha|T| $$\n",
    "       \n",
    "    is as small as possible. |T| corresponds to the number of terminal nodes of the tree T (its' complexity).\n",
    "        - When $\\alpha = 0, T = T_{0}$ \n",
    "        - As $\\alpha$ increases, there is a price to pay for having a tree with many terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d6fef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3516c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MODE** of all training observations in RJ (each observation belongs to the most commonly occuring class of training observations in the region to which it belongs).\n",
    "\n",
    "- The goal is to find boxes that minimize either the:\n",
    "\n",
    "    **Classification error rate** = the fraction of the training obs in that region that do not belong to the most common class.\n",
    "    \n",
    "    $$\\large E = 1-max(\\hat{p}_{mk})$$ \n",
    "    \n",
    "    where $\\hat{p}_{mk}$ is the proportion of the training obs in the mth region that are from the kth class.\n",
    "    \n",
    "    Or the\n",
    "    **Gini index** = the measure of the total variance across the k classes (or the node purity) (small G value implies \"pure\" node). Values range from 0 to 0.5.\n",
    "    \n",
    "    $$G = \\large\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})$$ \n",
    "    \n",
    "    Or the **Entropy** = measure of uncertainty. Values range from 0 to 1.\n",
    "    \n",
    "    $$D = \\large - \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log({p}_{mk})$$ \n",
    "\n",
    "- Once you've calculated the gini index or entropy for both branches of a node, we can determine the quality of the split by weighting the entropy of each branch by how many elements it has.\n",
    " \n",
    "     Information gain is based on the decrease in entropy after a dataset is split on an attribute. It helps to determine the order of attributes in the nodes of a decision tree (by finding the attribute that returns the highest information gain).\n",
    "     \n",
    "     **Information Gain = how much Entropy we removed** $= D_{before} - D_{after}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b936a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Gini, entropy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ddce8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "i.e. in a two class problem with 400 obs in each class, suppose one split creates nodes (300,100) and (100, 300) while the other creates nodes (200,400) and (200,0). Both splits have a misclassification rate of 0.25 but the second split produces a pure node and is probably preferable (it has a lower gini index and entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cbdb0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 1: 4 Red / 0 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d754a6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (prob(red)^2 + prob(blue)^2) = 1 - (1^2 + 0) = 0$\n",
    "$Entropy = -[prob(red)*log(prob(red))]-[prob(blue)*log(prob(blue))] = -[4/4*log(prob(4/4))]- 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efbe24",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 2 Red / 2 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444d21e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.5^2 + 0.5^2) = 0.5$ (the group is as impure as possible! (trick: divide answer by 0.5 --> 1)\n",
    "$Entropy = -[2/4*log(prob(2/4))]-[2/4*log(prob(2/4))] = -(-1/2)-(-1/2) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ca483",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 3 Red / 1 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d449",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.75^2 + 0.25^2) = 0.375$ (0.375/0.5 = 0.75 <-- prob of incorrect/correct labelling)\n",
    "$Entropy = -[3/4*log(prob(3/4))]-[1/4*log(prob(1/4))] = 0.811$ (a bit worse than gini score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b9acc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Good links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd3373",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/\n",
    "- https://victorzhou.com/blog/information-gain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f939723",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994fdf3",
   "metadata": {},
   "source": [
    "= the training data is unlabeled and the system tries to learn without a \"teacher\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed7ca0",
   "metadata": {},
   "source": [
    "# Semisupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c127d",
   "metadata": {},
   "source": [
    "= the training data is both labeled and unlabeled (usually a lot of unlabeled data and a little bit of labeled). i.e. Google photos: once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1,5 and 11 etc. If you tell the system who each person is, the system is able to name everyone in every photo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa49a4c",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a142254",
   "metadata": {},
   "source": [
    "= the learning system observes the environment, selects and performs actions, and gets rewards in return (or penalties in the form of negative rewards); it must then learn by itself what is the best strategy to get the most reward over time. i.e. DeepMind's AlphaGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998aa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
