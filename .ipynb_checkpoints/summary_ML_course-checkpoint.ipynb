{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa69123e",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9458e",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6a774",
   "metadata": {},
   "source": [
    "Decision trees involve segmenting the predictor space into a number of distinct and non-overlapping regions (R1, R2, ... RJ). Each split of the domain is aligned with one of the feature axes (in theory they could have any shape but \"rectangles\" are chosen for simplicity's sake and ease of interpretation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c651c0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34032c1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MEAN** of all training observations in RJ (the mean of the training observation in each partition is represented by a leaf).\n",
    "\n",
    "\n",
    "- The goal is to find boxes that minimize **the Residual Sum of Squares (RSS)**. \n",
    "- The **RSS** is computed by summing, for each test observation and across all partitions of the feature space, the squared difference of the response $y_{i}$ of a particular testing observation with the mean response of the training observation within the $j_{th}$ region. \n",
    "\n",
    "$$ \\large RSS = \\sum\\limits_{j=1} ^{J} \\sum\\limits_{i \\in R_{j}} (y_{i} - \\hat{y}_{R_{J}})^2 $$\n",
    "\n",
    "\n",
    "- Since it is computationally expensive to consider all possible partitions of the feature space into J rectangles, minimizing the RSS is done with **the Recursive Binary Splitting approach (RBS)**.\n",
    "\n",
    "\n",
    "- In one sentence: the RBS approach helps construct a tree by considering all features (X1, ... Xp) and all possible values of the cutpoint s for each of the features, and then choosing at each node the feature that best splits the data. It is said to be: \n",
    "    - *top down* because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space (each split --> new branch). \n",
    "    - *greedy* because at each step of the building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "\n",
    "- The RSS in greater detail: \n",
    "\n",
    "  For any j and s, a pair of half planes is defined (1). We then seek the values of j and s that minimize the equation (2) . \n",
    "  This process is repeated. However, each time, instead of splitting the entire predictor/feature space, only one of the two previously identified regions is split. The process continues until a stopping criterion is reached (i.e. until no region contains more than five observations).\n",
    "    1. \n",
    "    $$ \\large R_{1(j,s)} = \\{X | X_{j} < s\\} $$\n",
    "    $$ \\large R_{2(j,s)} = \\{X | X_{j} \\geqslant s\\} $$\n",
    "    \n",
    "    2. \n",
    "    \n",
    "$$ \\large\\sum\\limits_{i: x_{i} \\in R_{1(j,s)}} (y_{i} - \\hat{y}_{R_{1}})^2 + \\sum\\limits_{i: x_{i} \\in R_{2(j,s)}} (y_{i} - \\hat{y}_{R_{2}})^2 $$\n",
    "\n",
    "- In order to **NOT overfit** the data you could:\n",
    "\n",
    "    - Alternative 1: Build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. However, this strategy is too short-sighted since a \"worthless\" split might be followed by a \"good\" split later on.\n",
    "   \n",
    "    - Alternative 2: Grow a very large tree and then prune it back in order to obtain a subtree.\n",
    "    \n",
    "   **Cost complexity pruning** : introduces an additional tuning parameter ($\\alpha$) that balances the depth of the tree and the goodness of fit to training data. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_{0}$ such that:\n",
    "   \n",
    "       $$ \\large \\sum\\limits_{n=1} ^{|T|} \\sum\\limits_{x_{i} \\in R_{J}} (y_{i} - \\hat{y}_{R_{J}})^2 + \\alpha|T| $$\n",
    "       \n",
    "    is as small as possible. |T| corresponds to the number of terminal nodes of the tree T (its' complexity).\n",
    "        - When $\\alpha = 0, T = T_{0}$ \n",
    "        - As $\\alpha$ increases, there is a price to pay for having a tree with many terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d6fef",
   "metadata": {},
   "source": [
    "### Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3516c",
   "metadata": {},
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MODE** of all training observations in RJ (each observation belongs to the most commonly occuring class of training observations in the region to which it belongs).\n",
    "\n",
    "- The goal is to find boxes that minimize either the:\n",
    "\n",
    "    **Classification error rate** = the fraction of the training obs in that region that do not belong to the most common class.\n",
    "    \n",
    "    $$\\large E = 1-max(\\hat{p}_{mk})$$ \n",
    "    \n",
    "    where $\\hat{p}_{mk}$ is the proportion of the training obs in the mth region that are from the kth class.\n",
    "    \n",
    "    Or the\n",
    "    **Gini index** = the measure of the total variance across the k classes (or the node purity) (small G value implies \"pure\" node). Values range from 0 to 0.5.\n",
    "    \n",
    "    $$G = \\large\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})$$ \n",
    "    \n",
    "    Or the **Entropy** = measure of uncertainty. Values range from 0 to 1.\n",
    "    \n",
    "    $$D = \\large - \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log({p}_{mk})$$ \n",
    "\n",
    "- Once you've calculated the gini index or entropy for both branches of a node, we can determine the quality of the split by weighting the entropy of each branch by how many elements it has.\n",
    " \n",
    "     Information gain is based on the decrease in entropy after a dataset is split on an attribute. It helps to determine the order of attributes in the nodes of a decision tree (by finding the attribute that returns the highest information gain).\n",
    "     \n",
    "     **Information Gain = how much Entropy we removed** $= D_{before} - D_{after}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b936a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Gini, entropy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ddce8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "i.e. in a two class problem with 400 obs in each class, suppose one split creates nodes (300,100) and (100, 300) while the other creates nodes (200,400) and (200,0). Both splits have a misclassification rate of 0.25 but the second split produces a pure node and is probably preferable (it has a lower gini index and entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cbdb0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 1: 4 Red / 0 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d754a6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (prob(red)^2 + prob(blue)^2) = 1 - (1^2 + 0) = 0$\n",
    "$Entropy = -[prob(red)*log(prob(red))]-[prob(blue)*log(prob(blue))] = -[4/4*log(prob(4/4))]- 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efbe24",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 2 Red / 2 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444d21e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.5^2 + 0.5^2) = 0.5$ (the group is as impure as possible! (trick: divide answer by 0.5 --> 1)\n",
    "$Entropy = -[2/4*log(prob(2/4))]-[2/4*log(prob(2/4))] = -(-1/2)-(-1/2) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ca483",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 3 Red / 1 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d449",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.75^2 + 0.25^2) = 0.375$ (0.375/0.5 = 0.75 <-- prob of incorrect/correct labelling)\n",
    "$Entropy = -[3/4*log(prob(3/4))]-[1/4*log(prob(1/4))] = 0.811$ (a bit worse than gini score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b9acc",
   "metadata": {},
   "source": [
    "### Good links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd3373",
   "metadata": {},
   "source": [
    "- https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/\n",
    "- https://victorzhou.com/blog/information-gain/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
