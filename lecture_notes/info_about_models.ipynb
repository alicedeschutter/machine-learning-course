{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08501c4",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72423142",
   "metadata": {},
   "source": [
    "= the training data you feed to the algorithm includes the desired solutions (called labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac359ed",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab89e29",
   "metadata": {},
   "source": [
    "Many fancy statistical learning approaches can be seen as generalizations or extensions of linear\n",
    "regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d72897",
   "metadata": {},
   "source": [
    "### Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c649ad",
   "metadata": {},
   "source": [
    "A method for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. A **regression of Y on X** is expressed as: \n",
    "\n",
    "$$ \\large Y \\approx \\beta_{0} + \\beta_{1}X $$\n",
    "\n",
    "Where $\\beta_{0}$ and $\\beta_{1}$ are known as the model coefficients or parameters. \n",
    "\n",
    "To find $\\hat{y}$ the prediction of $Y$ on the basis of $X = x$, the equation below is used: \n",
    "\n",
    "$$ \\large \\hat{y} \\approx \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x $$ \n",
    "\n",
    "The hat symbol denotes the estimated value for an unknown parameter, or the predicted value of the response. The equation is known as the **least square line** since $\\beta_{0}$ and $\\beta_{1}$ are usually found by **minimizing the least squares criterion** (/minimizing the **Residual Sum of Squares (RSS)**):\n",
    "\n",
    "$$ RSS = (y_{1} - \\hat{y_{1}})^2 + (y_{2} - \\hat{y_{2}})^2 ... + (y_{n} - \\hat{y_{n}})^2 =(y_{1} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{1})^2 + (y_{2} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{2})^2 ... + (y_{n} - \\hat{\\beta_{0}} - \\hat{\\beta_{1}}x_{n})^2\n",
    " $$\n",
    " \n",
    "   The RSS is used as a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. \n",
    "    \n",
    "The equation that is used to express the best linear approximation to the true relationship between X and Y is known as the **population regression line**:\n",
    "\n",
    "$$ \\large Y = \\beta_{0} + \\beta_{1}X + \\epsilon$$\n",
    "\n",
    "In real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41e6a9",
   "metadata": {},
   "source": [
    "### Central limit theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb1730",
   "metadata": {},
   "source": [
    "Analogy between linear regression and estimation of the mean of a random variable (the analogy is based on bias): \n",
    "\n",
    "When we use the sample mean $\\mu$ to estimate the real population mean $\\hat{\\mu}$ the estimation is unbiased in the sense that, on average, we expect $\\mu$ to equal $\\hat{\\mu}$. For some samples, $\\mu$ might overestimate $\\hat{\\mu}$ and for others, $\\mu$ might underestimate $\\hat{\\mu}$. *An unbiased estimator does not systematically\n",
    "over- or under-estimate the true parameter.* \n",
    "\n",
    "The same is true for the coefficients $\\beta_{0} and \\beta_{1}$. The average of many least squares lines (with different $\\beta_{0} and \\beta_{1}$), each estimated from a separate data set, is pretty close to the true population regression line.\n",
    "\n",
    "The **standard error** of $\\mu$ tells us the average amount that the estimate $\\hat{\\mu}$ differs from the actual value of $\\mu$. The standard error is computed as follows:\n",
    "\n",
    "$$ SE(\\hat{\\mu}) = \\sqrt{VAR(\\hat{\\mu})} = \\sqrt{\\frac{\\sigma^2}{n}} $$\n",
    "\n",
    "Where $\\sigma$ is the **standard deviation** (a representation of the spread of each of the data points).\n",
    "\n",
    "*The standard deviation measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean measures how far the sample mean (average) of the data is likely to be from the true population mean.* \n",
    "**The standard error of a statistic is the approximate standard deviation of a statistical sample population**. \n",
    "\n",
    "The larger the sample size, the smaller the standard error because the statistic will approach the actual value. \n",
    "\n",
    "Similarly,the standard error associated with $\\beta_{0} and \\beta_{1}$ can easily be computed. Note that, in general, $\\sigma^2$ is not known but can be estimated from the data. The estimate is known as the **residual standard error** and is given by $RSE = \\sqrt{\\frac{RSS}{(n-2)}}$\n",
    "\n",
    "Standard errors can be used to:\n",
    "\n",
    "- Compute **confidence intervals**. A 95% confidence interval is a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. *If we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter.* \n",
    "\n",
    "    For linear regression, a 95% confidence interval for $\\beta_{0} and \\beta_{1}$ is computed as follows:\n",
    "\n",
    "    $$ \\hat{\\beta_{1}} \\pm 2 \\cdot SE(\\hat{\\beta_{1}})$$\n",
    "    $$ \\hat{\\beta_{0}} \\pm 2 \\cdot SE(\\hat{\\beta_{0}})$$\n",
    "\n",
    "    i.e. if the confidence interval for $\\beta_{0}$ is $[6.130, 7.935]$ and for $\\beta_{1}$ $[4.2, 5.3]$ for a given feature, we can conclude that in the absence of the feature, the response will be between 6.130 and 7.935 units. In addition, for each unit of increase for the feature, there will be an average increase in the response of between 4.2 and 5.3 units.\n",
    "    \n",
    "    \n",
    "- Perform **hypothesis testing** on the coefficients (null hyp: no relationship between X and Y (or $\\beta_{1} = 0$), alt hyp: there is a relationship (or $\\beta_{1} \\neq 0$)). If $\\hat{\\beta_{1}}$ is sufficiently far from 0 we can be confident that $\\beta_{1}$ is non-zero. \n",
    "\n",
    "    To assess what \"sufficiently\" means we look at the standard error of $\\hat{\\beta_{1}}$: When the SE($\\hat{\\beta_{1}}$ is small, then small values of $\\hat{\\beta_{1}}$ can provide strong evidence that $\\beta_{1} \\neq 0$. On the other hand, when SE($\\hat{\\beta_{1}}$ is large, then $|\\hat{\\beta_{1}}|$ must be large in order for us to reject the null hypothesis.\n",
    "    \n",
    "    The **t-statistic** measures the number of standard deviations that $\\hat{\\beta_{1}}$ is away from 0. The **p-value** is the probability of observing a value equal to |t| or larger (in abs value), assuming that the null hypothesis is true. \n",
    "    large t-value --> $\\hat{\\beta_{1}}$ far away from 0 --> low p-value (since the prob of observing a value that is so far from 0 assuming that it should be equal to 0 is low) --> reject null hypothesis (there is an association between the predictor and the response)! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f4ba5",
   "metadata": {},
   "source": [
    "### Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85cc98",
   "metadata": {},
   "source": [
    "If you have p distinct predictors, the multiple linear regression model takes the form:\n",
    "\n",
    "$$ \\large Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p} +\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7c87b",
   "metadata": {},
   "source": [
    "We interpret $\\beta_{j}$ as the average effect on Y of a one unit increase in $X_{j}$, holding all other predictors fixed. All of the $\\beta$ coefficients are estimated using the same least squares approach as for the simple linear regression. \n",
    "\n",
    "- **Obs.** The simple and multiple regression coefficients can be quite different:\n",
    "\n",
    "    *Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and more shark attacks. A multiple regression of shark attacks onto ice cream sales and temperature reveals that ice cream sales is no longer a significant predictor after adjusting for temperature.*\n",
    "\n",
    "\n",
    "- **Hypothesis testing**: for multiple linear regression, the null hypothesis is equal to $\\beta_{1} = \\beta_{1} = \\beta_{2} = .. = \\beta_{p} = 0$ and the alternative hypothesis is that at least one $\\beta_{j} \\neq 0$. This hypothesis test is performed by computing the **F -statistic**. A large value for the F-statistic implies that at least one of the features must be related to the response. A p-value is associated with the F-stat. \n",
    "\n",
    "    Why look at the p-value associated with the F-stat and not the individual p-values associated with the t-stat? \n",
    "    \n",
    "    *You would think that if any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response. However, this logic is flawed, especially when the number of predictors p is large! The F-statistic adjusts for the number of predictors but the individual p-values don't.* \n",
    "    \n",
    "    When is the F-stat reliable?\n",
    "    \n",
    "    *The F-stat is only reliable when n>p (low dimensional setting where n = number obs and p = number features). In addition, it is more reliable when n is large than when n is small.*\n",
    "    \n",
    "\n",
    "- **Variable selection**: Once you have concluded, on the basis of the p-value associated with the F-stat, that at least one predictor is associated with the response, how do you find which predictor?\n",
    "\n",
    "    Ideally, you try out different models, each containing a different subset of the predictors. However, there is about $2^p$ models that contain a subset of the predictors, so trying out every possible subset of the predictors is often infeasible. There exists three other approaches for variable selection:\n",
    "    1. **Forward Selection**: We begin with a model with an intercept but no predictors (a null model) and we fit p simple linear regressions. We add to the model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied. *Contrarily to backward selection, forward selection can always be used. However, forward selection is a greedy approach, and might include variables early that later become redundant.*\n",
    "    2. **Backward Selection**: We begin with all the variables in the model and remove the variable with the largest p-value. The new (p-1) model is fit and again, the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. *Backward selection can not be used if p>n.*\n",
    "    3. **Mixed Selection**: We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model.\n",
    "    \n",
    "    \n",
    "- **Terminology**: \n",
    "    The **least squares plane** (1) is only an estimate for the true **population regression plane** (2):\n",
    "    $$ \\large \\hat{Y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2} + ... + \\hat{\\beta_{p}}X_{p} \\qquad\\small (1) $$\n",
    "    $$ \\large f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p} \\qquad\\small (2) $$\n",
    "    \n",
    "    The inaccuracy in the coefficient estimates is related to the **reducible error**. Of course, in practice assuming a linear model for f (X) is almost always an approximation of reality, so there is an additional source of potentially reducible error which we call **model bias**.\n",
    "    \n",
    "    Even if we knew the true values for $\\beta_{0}, \\beta_{1}$ .. etc, the response value cannot be predicted perfectly because of the random error in the model $\\epsilon$, **the irreducible error.** \n",
    "    \n",
    "    **Prediction intervals** must account for both the uncertainty in estimating the population mean, plus the random variation of the individual values. So a prediction interval is always wider than a confidence interval.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750da0d7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assessing the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5146f1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The quality of a linear regression fit is typically assessed using two related quantities: **the residual standard error ($RSE$) and the $R^2$ statistic.**\n",
    "\n",
    "The $RSE$ is an estimate of the standard deviation of $\\epsilon$. You can see it as the average amount that the response will deviate from the true regression line.\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{(n-p-1)}}$$ \n",
    "\n",
    "The $RSE$ is considered a measure of the lack of fit of the model to the data. A large $RSE$ indicates that the model doesn't fit the data very well. For simple linear regressions, p = 1 and thus $RSS$ is divided by (n-2).\n",
    "\n",
    "Since $RSE$ is measured in the units of Y, it is not always clear what a good $RSE$ constitutes. The $R^2$ statistic provides an alternative measure of fit. It measures the proportion of variability in Y that can be explained using X. An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response is\n",
    "explained by the regression.\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "- **Obs 1**: in typical applications in biology, psychology, etc. the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an $R^2$ value well below 0.1 might be more realistic than a large one.\n",
    "- **Obs 2**: In the simple linear regression setting, the squared correlation and the $R^2$ statistic are similar: $R^2 = Cor(X, Y)^2$, and, in the multiple linear regression setting, $R^2$ is equal to the square of the correlation between the response and the fitted linear model: $R^2 = Cor(Y, \\hat{Y})^2$\n",
    "- **Obs 3**: $R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. Why? Adding another variable results in a decrease in the RSS on the training data (though not necessarily the testing data). Thus, the $R^2$ statistic, which is also computed on the training data, must increase. \n",
    "    On the other hand, the $RSE$ doesn't always decrease when more variables are added to the model. Why? Models with more variables can have higher $RSE$ if the decrease in $RSS$ is small relative to the increase in p.\n",
    "    \n",
    "Finally, it can be useful to plot the data. Graphical summaries can reveal problems with a model that are not visible from numerical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8ee4d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dbdbc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Algorithm that makes classification predictions about your desired data points based on the assumption that nearby data points are similar to your test point.\n",
    "\n",
    "The k-nearest neighbors algorithm hinges on data points being close together. This becomes challenging as the number of dimensions increases, referred to as the “Curse of Dimensionality.” It’s especially hard for the k-nearest neighbors algorithm it requires two points to be very close on every axis, and adding a new dimension creates another opportunity for points to be farther apart. As the number of dimensions increases, the closest distance between two points approaches the average distance between points, eradicating the ability of the k-nearest neighbors algorithm to provide valuable predictions.\n",
    "\n",
    "To overcome this challenge, you can add more data to the data set. By doing so you add density to the data space, bringing the nearest points closer together and returning the ability of the k-nearest neighbors algorithm to provide valuable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9458e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6a774",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decision trees involve segmenting the predictor space into a number of distinct and non-overlapping regions (R1, R2, ... RJ). Each split of the domain is aligned with one of the feature axes (in theory they could have any shape but \"rectangles\" are chosen for simplicity's sake and ease of interpretation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c651c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34032c1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MEAN** of all training observations in RJ (the mean of the training observation in each partition is represented by a leaf).\n",
    "\n",
    "\n",
    "- The goal is to find boxes that minimize **the Residual Sum of Squares (RSS)**. \n",
    "- The **RSS** is computed by summing, for each test observation and across all partitions of the feature space, the squared difference of the response $y_{i}$ of a particular testing observation with the mean response of the training observation within the $j_{th}$ region. \n",
    "\n",
    "$$ \\large RSS = \\sum\\limits_{j=1} ^{J} \\sum\\limits_{i \\in R_{j}} (y_{i} - \\hat{y}_{R_{J}})^2 $$\n",
    "\n",
    "\n",
    "- Since it is computationally expensive to consider all possible partitions of the feature space into J rectangles, minimizing the RSS is done with **the Recursive Binary Splitting approach (RBS)**.\n",
    "\n",
    "\n",
    "- In one sentence: the RBS approach helps construct a tree by considering all features (X1, ... Xp) and all possible values of the cutpoint s for each of the features, and then choosing at each node the feature that best splits the data. It is said to be: \n",
    "    - *top down* because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space (each split --> new branch). \n",
    "    - *greedy* because at each step of the building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "\n",
    "- The RSS in greater detail: \n",
    "\n",
    "  For any j and s, a pair of half planes is defined (1). We then seek the values of j and s that minimize the equation (2) . \n",
    "  This process is repeated. However, each time, instead of splitting the entire predictor/feature space, only one of the two previously identified regions is split. The process continues until a stopping criterion is reached (i.e. until no region contains more than five observations).\n",
    "    1. \n",
    "    $$ \\large R_{1(j,s)} = \\{X | X_{j} < s\\} $$\n",
    "    $$ \\large R_{2(j,s)} = \\{X | X_{j} \\geqslant s\\} $$\n",
    "    \n",
    "    2. \n",
    "    \n",
    "$$ \\large\\sum\\limits_{i: x_{i} \\in R_{1(j,s)}} (y_{i} - \\hat{y}_{R_{1}})^2 + \\sum\\limits_{i: x_{i} \\in R_{2(j,s)}} (y_{i} - \\hat{y}_{R_{2}})^2 $$\n",
    "\n",
    "- In order to **NOT overfit** the data you could:\n",
    "\n",
    "    - Alternative 1: Build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. However, this strategy is too short-sighted since a \"worthless\" split might be followed by a \"good\" split later on.\n",
    "   \n",
    "    - Alternative 2: Grow a very large tree and then prune it back in order to obtain a subtree.\n",
    "    \n",
    "   **Cost complexity pruning** : introduces an additional tuning parameter ($\\alpha$) that balances the depth of the tree and the goodness of fit to training data. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_{0}$ such that:\n",
    "   \n",
    "       $$ \\large \\sum\\limits_{n=1} ^{|T|} \\sum\\limits_{x_{i} \\in R_{J}} (y_{i} - \\hat{y}_{R_{J}})^2 + \\alpha|T| $$\n",
    "       \n",
    "    is as small as possible. |T| corresponds to the number of terminal nodes of the tree T (its' complexity).\n",
    "        - When $\\alpha = 0, T = T_{0}$ \n",
    "        - As $\\alpha$ increases, there is a price to pay for having a tree with many terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d6fef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3516c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The predictor/feature space is segmented into distinct, non-overlapping regions (R1... RJ).\n",
    "- Any new observation that falls into a particular partition RJ has the estimated response given by the **MODE** of all training observations in RJ (each observation belongs to the most commonly occuring class of training observations in the region to which it belongs).\n",
    "\n",
    "- The goal is to find boxes that minimize either the:\n",
    "\n",
    "    **Classification error rate** = the fraction of the training obs in that region that do not belong to the most common class.\n",
    "    \n",
    "    $$\\large E = 1-max(\\hat{p}_{mk})$$ \n",
    "    \n",
    "    where $\\hat{p}_{mk}$ is the proportion of the training obs in the mth region that are from the kth class.\n",
    "    \n",
    "    Or the\n",
    "    **Gini index** = the measure of the total variance across the k classes (or the node purity) (small G value implies \"pure\" node). Values range from 0 to 0.5.\n",
    "    \n",
    "    $$G = \\large\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})$$ \n",
    "    \n",
    "    Or the **Entropy** = measure of uncertainty. Values range from 0 to 1.\n",
    "    \n",
    "    $$D = \\large - \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log({p}_{mk})$$ \n",
    "\n",
    "- Once you've calculated the gini index or entropy for both branches of a node, we can determine the quality of the split by weighting the entropy of each branch by how many elements it has.\n",
    " \n",
    "     Information gain is based on the decrease in entropy after a dataset is split on an attribute. It helps to determine the order of attributes in the nodes of a decision tree (by finding the attribute that returns the highest information gain).\n",
    "     \n",
    "     **Information Gain = how much Entropy we removed** $= D_{before} - D_{after}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b936a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Gini, entropy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ddce8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "i.e. in a two class problem with 400 obs in each class, suppose one split creates nodes (300,100) and (100, 300) while the other creates nodes (200,400) and (200,0). Both splits have a misclassification rate of 0.25 but the second split produces a pure node and is probably preferable (it has a lower gini index and entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cbdb0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 1: 4 Red / 0 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d754a6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (prob(red)^2 + prob(blue)^2) = 1 - (1^2 + 0) = 0$\n",
    "$Entropy = -[prob(red)*log(prob(red))]-[prob(blue)*log(prob(blue))] = -[4/4*log(prob(4/4))]- 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efbe24",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 2 Red / 2 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444d21e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.5^2 + 0.5^2) = 0.5$ (the group is as impure as possible! (trick: divide answer by 0.5 --> 1)\n",
    "$Entropy = -[2/4*log(prob(2/4))]-[2/4*log(prob(2/4))] = -(-1/2)-(-1/2) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ca483",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example 2: 3 Red / 1 Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d449",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Gini = 1 - (0.75^2 + 0.25^2) = 0.375$ (0.375/0.5 = 0.75 <-- prob of incorrect/correct labelling)\n",
    "$Entropy = -[3/4*log(prob(3/4))]-[1/4*log(prob(1/4))] = 0.811$ (a bit worse than gini score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b9acc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Good links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd3373",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/\n",
    "- https://victorzhou.com/blog/information-gain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f939723",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994fdf3",
   "metadata": {},
   "source": [
    "= the training data is unlabeled and the system tries to learn without a \"teacher\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed7ca0",
   "metadata": {},
   "source": [
    "# Semisupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c127d",
   "metadata": {},
   "source": [
    "= the training data is both labeled and unlabeled (usually a lot of unlabeled data and a little bit of labeled). i.e. Google photos: once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1,5 and 11 etc. If you tell the system who each person is, the system is able to name everyone in every photo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa49a4c",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a142254",
   "metadata": {},
   "source": [
    "= the learning system observes the environment, selects and performs actions, and gets rewards in return (or penalties in the form of negative rewards); it must then learn by itself what is the best strategy to get the most reward over time. i.e. DeepMind's AlphaGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998aa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
