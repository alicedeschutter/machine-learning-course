{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17585f5",
   "metadata": {},
   "source": [
    "**General Machine Learning Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e5026",
   "metadata": {},
   "source": [
    "**Machine learning = science of programming computers so they can learn from data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74cfa9",
   "metadata": {},
   "source": [
    "# Statistical learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67bca2",
   "metadata": {},
   "source": [
    "**Statistical learning = a set of approaches for estimating f.**\n",
    "\n",
    "$$\\large Y = f(X) + \\epsilon$$ \n",
    "\n",
    "where $\\epsilon$ is a random error term which is independent of X and has mean zero, f represents the systematic information that X provides about Y.\n",
    "\n",
    "**X:** input variables (predictors/ features/ independent variables)\n",
    "\n",
    "**Y:** output variable (response/ dependent variable)\n",
    "\n",
    "Main reasons for estimating f?\n",
    "- **Prediction**: \n",
    "\n",
    "    - When a set of inputs X is readily available, but the output Y can not easily be obtained. \n",
    "    - Since $\\epsilon$ averages to zero, Y is predicted using:\n",
    "\n",
    "    $ \\hat{Y} = \\hat{f}(X) $, where $\\hat{f}$ is the estimate for $f$ and $\\hat{Y}$ is the resulting prediction. \n",
    "    - Here, we are not concerned with the exact form of $\\hat{f}$ (it's a black box).\n",
    "    - There are two types of errors: \n",
    "        - **the reducible error** ($\\hat{f}$ is not a perfect estimate of $\\hat{f}$) \n",
    "        - **the irreducible error** (the existence of $\\epsilon$ > 0), this error provides an upper bound on the accuracy of our predictions for Y.\n",
    "\n",
    "\n",
    "- **Inference**:\n",
    "    - When trying to understand the association between $Y$ and $X_{1}, X_{2}, X_{3}, ...$\n",
    "    \n",
    "There are two types of statistical learning methods:\n",
    "\n",
    "- **Parametric**: It reduces the problem of estimating $f$ down to one of estimating a set of\n",
    "parameters. How? First you make an assumption about the function form (i.e. it is linear), then you fit (or train) the chosen model (i.e. with the ordinary least squares method).\n",
    "\n",
    "\n",
    "- **Non-parametric**: do not make explicit assumptions about the functional form of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly. Disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.\n",
    "\n",
    "Difference between Regression and Classification problems:\n",
    "\n",
    "- **Regressions** require a quantitative response.\n",
    "- **Classifications** require a qualitative response (logistic regression falls into this category!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bccbfa",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d00fb1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b3b41",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For regressions, the most commonly used quality of fit method is the mean squared error (MSE). \n",
    "\n",
    "$$\\large MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (y_{i} - \\hat{f}(x_{i}))^2$$\n",
    "\n",
    "Here, the MSE is computed using the training data, so it is referred to the training MSE. Since we are interested in the accuracy of the predictions we should instead compute the test MSE (the average squared prediction error for these test observations $(x_{0}, y_{0})$):\n",
    "\n",
    "$$\\large Ave (y_{0} - \\hat{f}(x_{0}))^2$$\n",
    "\n",
    "Obs: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE!\n",
    "\n",
    "The expected test MSE for a given value $x_{0}$ can always be decomposed into the sum of three fundamental quantities: the variance of $\\hat{f}(x_{0})$, the squared bias of $\\hat{f}(x_{0})$, and the variance of the error term $\\epsilon$. \n",
    "\n",
    "   Since both the $Var(\\hat{f}(x_{0}))$ and the $Bias(\\hat{f}(x_{0}))^2$ are both non negative terms, the expected test MSE can never lie below $Var(\\epsilon)$\n",
    "    \n",
    "   **BIAS-VARIANCE TRADE-OFF**\n",
    "   - **The variance = amount by which $\\hat{f}(x_{0})$ would change if we estimated it using a different training data set.**\n",
    "   - **Bias = error that is introduced by approximating a real life problem by a much simpler model.**\n",
    "   - In general, the more flexible the model, the higher the variance and the lower the bias. The bias tends to initially decrease more than the variance increases. That is why the test MSE usually decreases at first (until the variance starts to significantly increase)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7089ef",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f073d7a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The **bias-variance trade-off** transfers over to the classification setting. However, since $y_{i}$ is no longer quantitative, the accuracy of the estimate $\\hat{f}(x_{0})$ is given by the training error rate:\n",
    "\n",
    "$$\\large \\frac{1}{n} \\sum\\limits_{i=1}^n I(y_{i} \\neq \\hat{y_{i}})$$\n",
    "\n",
    "where $\\hat{y_{i}}$ is the predicted class label for the ith observation and $I(y_{i} \\neq \\hat{y_{i}})$ is an indicator variable that equals 1 if $y_{i} \\neq \\hat{y_{i}}$, and 0 if $y_{i} = \\hat{y_{i}}$.\n",
    "\n",
    "The test error rate is:\n",
    "\n",
    "$$\\large Ave(I(y_{0} \\neq \\hat{y_{0}})$$\n",
    "\n",
    "where $y_{0}$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_{0}$.\n",
    "\n",
    "This test error is minimized on average by **the Bayes Classifier** which assigns each observation to the most likely class, given its predictor values. The classifier always chooses the class for which $P(Y=j | x=x_{0})$ is the largest.\n",
    "\n",
    "The Bayes classifier produces the lowest possible test error rate. The Bayes error rate is:\n",
    "\n",
    "$$\\large 1 - E(max_j P(Y = j | X))$$\n",
    "\n",
    "In practice, you can't calculate the conditional distribution of Y given X because that distribution is defined over the entire population. The Bayes Classifier is the state of nature that we do not know but we aim to approximate as well as possible.\n",
    "Another method that attempts to estimate the conditional distribution of Y given X, and then classifies a\n",
    "given observation to the class with highest estimated probability is **the KNN Classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ebe48",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321dcc7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade7722",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. \n",
    "\n",
    "- **Validation set approach:**\n",
    "\n",
    "    Method: randomly dividing the available set of observations into two parts, a **training set** and a **validation set or hold-out set**. The model is fit on the training set, and the fitted model is used to predict the\n",
    "    responses for the observations in the validation set. The resulting validation set error rate provides an estimate of the test error rate.\n",
    "\n",
    "    **Drawbacks:**\n",
    "    1. Highly variable validation estimate of the test error rate (depending on which observations are included in the training/validation sets).\n",
    "    2. Only a subset of the observations (those that are included in the training set) are used to fit the model. The validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\n",
    "\n",
    "\n",
    "- **Leave one out approach (LOOCV):**\n",
    "\n",
    "    Method: splits the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $(x_{1}, y_{1})$ is used for the validation set, and the remaining observations make up the training set( (n-1) observations). $MSE_1 = (y_{1} − y_{1})^2$ provides an approximately unbiased estimate for the test error. This procedure is then repeated n times and the estimate for the test MSE is: $$CV_n = \\frac{1}{n} \\sum\\limits_{i=1}^n MSE_i$$\n",
    "\n",
    "    **Advantages over validation set approach:**\n",
    "    1. Far less bias --> not to overestimate the test error rate as much.\n",
    "    2. Performing the LOOCV several times always yield the same results (no randomness in the training/ validation set splits.)\n",
    "\n",
    "    **Drawbacks:**\n",
    "    1. Time consuming if n is large and if each individual model is slow to fit.\n",
    "\n",
    "\n",
    "- **K-fold cross validation:**\n",
    "\n",
    "    Method: randomly ividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k−1$ folds. The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This is repeated k times, each time, a different group of observations is treated as a validation set. We then have k estimates for the test error. This final test estimate is: $$CV_k = \\frac{1}{k} \\sum\\limits_{i=1}^k MSE_i$$\n",
    "\n",
    "    **Advantages over LOOCV approach:**\n",
    "\n",
    "    1. Might be more feasible/ less time consuming.\n",
    "    2. Useful when the location of the minimum in the estimated MSE curve is of interest (not the actual value of the test MSE) (i.e. to identify the method that results in the lowest test error).\n",
    "    3. Often gives more accurate estimates of the test error rate than does LOOCV (although LOOCV has less bias than K-fold CV, LOOCV has higher variance). Why?\n",
    "        Because for the LOOCV, we are averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other (the mean of many highly correlated quantities has higher variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761dc1c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3b3d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cross-validation works just as described for the Regressions, except that rather than using MSE to quantify test error, we instead use the number of misclassified observations (see 1.1.2 and 1.1.3.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d13ed4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Considerations in high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805cb9f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most traditional statistical techniques are intended for the low-dimensional setting in which n, the number of observations, is much greater than p, the number of features. Classical approaches such as least squares linear regression are not appropriate for high-dimensional data sets. Why? when $p > n$ or $p ≈ n$, a simple least\n",
    "squares regression line is too flexible and overfits the data.\n",
    "\n",
    "Less flexible fitting approaches should instead be used (i.e. Lasso, PCR, Ridge)\n",
    "\n",
    "**Curse of dimentionality**: adding noise features that are not truly associated with the response will lead\n",
    "to a deterioration in the fitted model, and consequently an increased test set error (noise features increase the dimensionality of the problem, exacerbating the risk of overfitting).\n",
    "\n",
    "**Multicollinearity problem** is high for high-dimensional data sets: any variable in the model can be written as a linear combination of all of the other variables in the model which means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c160c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
